{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 0: Install dependencies =====\n",
        "!pip install -q ckip-transformers transformers torch tqdm pandas"
      ],
      "metadata": {
        "id": "jHSyiNbADvx9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== Step 1: Imports =====\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
        "from ckip_transformers.nlp import CkipWordSegmenter\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import math\n"
      ],
      "metadata": {
        "id": "wd6iGPg0DwVb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 2: Load models =====\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ckiplab/gpt2-base-chinese\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"ckiplab/gpt2-base-chinese\")\n",
        "ws_driver = CkipWordSegmenter(device=0 if torch.cuda.is_available() else -1)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85mDhuT0DyOk",
        "outputId": "dfe2c15c-5cbb-4784-c8c2-fd6fd58e8fc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(21128, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 3: Load stimuli =====\n",
        "stimuli = pd.read_csv(\"stimuli.csv\")  # Must have column \"sentence\""
      ],
      "metadata": {
        "id": "KVj9IoQSD3NQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 4: Word-level surprisal function =====\n",
        "def compute_word_surprisal(sentence):\n",
        "    seg_result = ws_driver([sentence])\n",
        "    words = seg_result[0]  # full word list\n",
        "    surprisals = []\n",
        "\n",
        "    context_words = []\n",
        "    for word in words:\n",
        "        context_text = \" \".join(context_words)\n",
        "        context_enc = tokenizer(context_text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "        context_ids = context_enc[\"input_ids\"]\n",
        "        if torch.cuda.is_available():\n",
        "            context_ids = context_ids.to(\"cuda\")\n",
        "\n",
        "        target_ids = tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(target_ids) == 0:\n",
        "            continue  # skip empty word\n",
        "\n",
        "        target_ids = torch.tensor(target_ids, dtype=torch.long).unsqueeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "            target_ids = target_ids.to(\"cuda\")\n",
        "\n",
        "        input_ids = torch.cat([context_ids, target_ids], dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        surprisal_sum = 0\n",
        "        for j in range(target_ids.shape[1]):\n",
        "            token_idx = context_ids.shape[1] + j - 1\n",
        "            tid = target_ids[0, j]\n",
        "            surprisal_sum += -log_probs[0, token_idx, tid].item() / math.log(2)\n",
        "\n",
        "        surprisals.append(surprisal_sum)\n",
        "        context_words.append(word)\n",
        "\n",
        "    return words, surprisals\n"
      ],
      "metadata": {
        "id": "PmSzlZhmD35h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 5: Compute surprisal for all sentences =====\n",
        "all_rows = []\n",
        "\n",
        "for i, row in tqdm(stimuli.iterrows(), total=len(stimuli), desc=\"Processing sentences\"):\n",
        "    sentence = str(row[\"sentence\"]).strip()\n",
        "    if not sentence:\n",
        "        continue\n",
        "    try:\n",
        "        words, surprisals = compute_word_surprisal(sentence)\n",
        "        for w, s in zip(words, surprisals):\n",
        "            all_rows.append({\n",
        "                \"sentence_id\": i,\n",
        "                \"sentence\": sentence,\n",
        "                \"word\": w,\n",
        "                \"surprisal\": s\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping row {i} due to error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPfvZo7YD5xb",
        "outputId": "3edab8aa-1203-4178-d906-665b4ffd824b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing sentences:   0%|          | 0/107 [00:00<?, ?it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n",
            "Processing sentences:   1%|          | 1/107 [00:00<01:24,  1.25it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
            "Processing sentences:   2%|▏         | 2/107 [00:01<01:19,  1.32it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5769.33it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
            "Processing sentences:   3%|▎         | 3/107 [00:02<01:17,  1.35it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6797.90it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
            "Processing sentences:   4%|▎         | 4/107 [00:03<01:16,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5833.52it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.48it/s]\n",
            "Processing sentences:   5%|▍         | 5/107 [00:03<01:16,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7133.17it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.21it/s]\n",
            "Processing sentences:   6%|▌         | 6/107 [00:04<01:16,  1.32it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 718.45it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
            "Processing sentences:   7%|▋         | 7/107 [00:05<01:15,  1.32it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7854.50it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
            "Processing sentences:   7%|▋         | 8/107 [00:06<01:14,  1.32it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n",
            "Processing sentences:   8%|▊         | 9/107 [00:06<01:13,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
            "Processing sentences:   9%|▉         | 10/107 [00:07<01:11,  1.35it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1370.24it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
            "Processing sentences:  10%|█         | 11/107 [00:08<01:14,  1.29it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6921.29it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
            "Processing sentences:  11%|█         | 12/107 [00:09<01:17,  1.22it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
            "Processing sentences:  12%|█▏        | 13/107 [00:10<01:21,  1.16it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8422.30it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
            "Processing sentences:  13%|█▎        | 14/107 [00:11<01:21,  1.14it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5584.96it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
            "Processing sentences:  14%|█▍        | 15/107 [00:11<01:16,  1.21it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.27it/s]\n",
            "Processing sentences:  15%|█▍        | 16/107 [00:12<01:13,  1.25it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
            "Processing sentences:  16%|█▌        | 17/107 [00:13<01:10,  1.27it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
            "Processing sentences:  17%|█▋        | 18/107 [00:13<01:05,  1.35it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6403.52it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
            "Processing sentences:  18%|█▊        | 19/107 [00:14<01:02,  1.41it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n",
            "Processing sentences:  19%|█▊        | 20/107 [00:15<01:02,  1.39it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6786.90it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
            "Processing sentences:  20%|█▉        | 21/107 [00:16<01:02,  1.37it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6909.89it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
            "Processing sentences:  21%|██        | 22/107 [00:16<01:02,  1.36it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5475.59it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
            "Processing sentences:  21%|██▏       | 23/107 [00:17<01:01,  1.36it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6269.51it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
            "Processing sentences:  22%|██▏       | 24/107 [00:18<00:59,  1.40it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6657.63it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
            "Processing sentences:  23%|██▎       | 25/107 [00:19<00:59,  1.38it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4228.13it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
            "Processing sentences:  24%|██▍       | 26/107 [00:19<00:59,  1.37it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1507.12it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
            "Processing sentences:  25%|██▌       | 27/107 [00:20<00:59,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
            "Processing sentences:  26%|██▌       | 28/107 [00:21<00:58,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7839.82it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
            "Processing sentences:  27%|██▋       | 29/107 [00:22<01:01,  1.26it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
            "Processing sentences:  28%|██▊       | 30/107 [00:23<01:04,  1.19it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
            "Processing sentences:  29%|██▉       | 31/107 [00:24<01:04,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1954.48it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
            "Processing sentences:  30%|██▉       | 32/107 [00:24<00:59,  1.27it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8050.49it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
            "Processing sentences:  31%|███       | 33/107 [00:25<00:56,  1.31it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n",
            "Processing sentences:  32%|███▏      | 34/107 [00:26<00:55,  1.31it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
            "Processing sentences:  33%|███▎      | 35/107 [00:26<00:54,  1.31it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4140.48it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
            "Processing sentences:  34%|███▎      | 36/107 [00:27<00:51,  1.38it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
            "Processing sentences:  35%|███▍      | 37/107 [00:28<00:52,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5197.40it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
            "Processing sentences:  36%|███▌      | 38/107 [00:29<00:52,  1.31it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
            "Processing sentences:  36%|███▋      | 39/107 [00:29<00:52,  1.30it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
            "Processing sentences:  37%|███▋      | 40/107 [00:30<00:51,  1.30it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.42it/s]\n",
            "Processing sentences:  38%|███▊      | 41/107 [00:31<00:50,  1.30it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
            "Processing sentences:  39%|███▉      | 42/107 [00:32<00:50,  1.28it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7724.32it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
            "Processing sentences:  40%|████      | 43/107 [00:33<00:50,  1.26it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4202.71it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
            "Processing sentences:  41%|████      | 44/107 [00:33<00:49,  1.26it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
            "Processing sentences:  42%|████▏     | 45/107 [00:34<00:52,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7319.90it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
            "Processing sentences:  43%|████▎     | 46/107 [00:35<00:53,  1.15it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2768.52it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
            "Processing sentences:  44%|████▍     | 47/107 [00:36<00:53,  1.12it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7157.52it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
            "Processing sentences:  45%|████▍     | 48/107 [00:37<00:50,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
            "Processing sentences:  46%|████▌     | 49/107 [00:38<00:47,  1.23it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7206.71it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
            "Processing sentences:  47%|████▋     | 50/107 [00:38<00:45,  1.25it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6932.73it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.52it/s]\n",
            "Processing sentences:  48%|████▊     | 51/107 [00:39<00:43,  1.28it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n",
            "Processing sentences:  49%|████▊     | 52/107 [00:40<00:40,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7449.92it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
            "Processing sentences:  50%|████▉     | 53/107 [00:41<00:40,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4215.38it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.96it/s]\n",
            "Processing sentences:  50%|█████     | 54/107 [00:41<00:39,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
            "Processing sentences:  51%|█████▏    | 55/107 [00:42<00:38,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5370.43it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
            "Processing sentences:  52%|█████▏    | 56/107 [00:43<00:38,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 504.12it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n",
            "Processing sentences:  53%|█████▎    | 57/107 [00:44<00:37,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n",
            "Processing sentences:  54%|█████▍    | 58/107 [00:44<00:36,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7854.50it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
            "Processing sentences:  55%|█████▌    | 59/107 [00:45<00:35,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6875.91it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
            "Processing sentences:  56%|█████▌    | 60/107 [00:46<00:35,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6462.72it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n",
            "Processing sentences:  57%|█████▋    | 61/107 [00:47<00:35,  1.29it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8525.01it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
            "Processing sentences:  58%|█████▊    | 62/107 [00:48<00:36,  1.22it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6647.07it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
            "Processing sentences:  59%|█████▉    | 63/107 [00:49<00:37,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8371.86it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
            "Processing sentences:  60%|█████▉    | 64/107 [00:49<00:36,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5866.16it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
            "Processing sentences:  61%|██████    | 65/107 [00:50<00:34,  1.22it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7503.23it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.96it/s]\n",
            "Processing sentences:  62%|██████▏   | 66/107 [00:51<00:32,  1.24it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n",
            "Processing sentences:  63%|██████▎   | 67/107 [00:52<00:31,  1.26it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3724.96it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
            "Processing sentences:  64%|██████▎   | 68/107 [00:52<00:30,  1.28it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
            "Processing sentences:  64%|██████▍   | 69/107 [00:53<00:29,  1.29it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
            "Processing sentences:  65%|██████▌   | 70/107 [00:54<00:28,  1.28it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3816.47it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\n",
            "Processing sentences:  66%|██████▋   | 71/107 [00:55<00:30,  1.20it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
            "Processing sentences:  67%|██████▋   | 72/107 [00:56<00:30,  1.14it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
            "Processing sentences:  68%|██████▊   | 73/107 [00:57<00:28,  1.18it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1025.00it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
            "Processing sentences:  69%|██████▉   | 74/107 [00:58<00:28,  1.15it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5966.29it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
            "Processing sentences:  70%|███████   | 75/107 [00:59<00:28,  1.13it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8019.70it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
            "Processing sentences:  71%|███████   | 76/107 [00:59<00:27,  1.13it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8456.26it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
            "Processing sentences:  72%|███████▏  | 77/107 [01:00<00:25,  1.17it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n",
            "Processing sentences:  73%|███████▎  | 78/107 [01:01<00:23,  1.22it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7345.54it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
            "Processing sentences:  74%|███████▍  | 79/107 [01:02<00:23,  1.22it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.69it/s]\n",
            "Processing sentences:  75%|███████▍  | 80/107 [01:03<00:21,  1.25it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
            "Processing sentences:  76%|███████▌  | 81/107 [01:03<00:19,  1.33it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.36it/s]\n",
            "Processing sentences:  77%|███████▋  | 82/107 [01:04<00:17,  1.39it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6186.29it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.20it/s]\n",
            "Processing sentences:  78%|███████▊  | 83/107 [01:04<00:15,  1.51it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.44it/s]\n",
            "Processing sentences:  79%|███████▊  | 84/107 [01:05<00:15,  1.51it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s]\n",
            "Processing sentences:  79%|███████▉  | 85/107 [01:06<00:13,  1.59it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
            "Processing sentences:  80%|████████  | 86/107 [01:06<00:12,  1.66it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 717.10it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
            "Processing sentences:  81%|████████▏ | 87/107 [01:07<00:12,  1.64it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6472.69it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
            "Processing sentences:  82%|████████▏ | 88/107 [01:07<00:10,  1.79it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n",
            "Processing sentences:  83%|████████▎ | 89/107 [01:08<00:09,  1.82it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 810.65it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n",
            "Processing sentences:  84%|████████▍ | 90/107 [01:08<00:09,  1.83it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8176.03it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]\n",
            "Processing sentences:  85%|████████▌ | 91/107 [01:09<00:09,  1.75it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6260.16it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
            "Processing sentences:  86%|████████▌ | 92/107 [01:09<00:08,  1.81it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6909.89it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
            "Processing sentences:  87%|████████▋ | 93/107 [01:10<00:07,  1.78it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6078.70it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
            "Processing sentences:  88%|████████▊ | 94/107 [01:11<00:07,  1.72it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
            "Processing sentences:  89%|████████▉ | 95/107 [01:11<00:07,  1.69it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7710.12it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
            "Processing sentences:  90%|████████▉ | 96/107 [01:12<00:06,  1.66it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6921.29it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.88it/s]\n",
            "Processing sentences:  91%|█████████ | 97/107 [01:13<00:06,  1.54it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8886.24it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
            "Processing sentences:  92%|█████████▏| 98/107 [01:13<00:06,  1.49it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8665.92it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
            "Processing sentences:  93%|█████████▎| 99/107 [01:14<00:05,  1.34it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
            "Processing sentences:  93%|█████████▎| 100/107 [01:15<00:05,  1.30it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7489.83it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
            "Processing sentences:  94%|█████████▍| 101/107 [01:16<00:04,  1.30it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
            "Processing sentences:  95%|█████████▌| 102/107 [01:17<00:03,  1.29it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5146.39it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n",
            "Processing sentences:  96%|█████████▋| 103/107 [01:17<00:03,  1.23it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
            "Processing sentences:  97%|█████████▋| 104/107 [01:18<00:02,  1.24it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6594.82it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.03it/s]\n",
            "Processing sentences:  98%|█████████▊| 105/107 [01:19<00:01,  1.26it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6786.90it/s]\n",
            "\n",
            "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
            "Processing sentences:  99%|█████████▉| 106/107 [01:20<00:00,  1.20it/s]\n",
            "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
            "\n",
            "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
            "Processing sentences: 100%|██████████| 107/107 [01:20<00:00,  1.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 6: Save CSV =====\n",
        "df_out = pd.DataFrame(all_rows)\n",
        "df_out.to_csv(\"stimuli_word_surprisal.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"✅ Done! CSV saved as 'stimuli_word_surprisal.csv'\")\n",
        "\n",
        "\n",
        "# ===== Step 7: Sanity check print =====\n",
        "print(df_out.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzbg3s6mRnAh",
        "outputId": "4aa25384-e8cc-4ee5-f09b-dd5c46a03bba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! CSV saved as 'stimuli_word_surprisal.csv'\n",
            "   sentence_id  sentence word  surprisal\n",
            "0            0  小明買了一杯咖啡   小明  20.341637\n",
            "1            0  小明買了一杯咖啡    買  13.261948\n",
            "2            0  小明買了一杯咖啡    了   6.256403\n",
            "3            0  小明買了一杯咖啡    一   6.303586\n",
            "4            0  小明買了一杯咖啡    杯  11.824003\n",
            "5            0  小明買了一杯咖啡   咖啡   9.538607\n",
            "6            1  小明喝了一杯奶茶   小明  20.341637\n",
            "7            1  小明喝了一杯奶茶    喝  16.194701\n",
            "8            1  小明喝了一杯奶茶    了   5.740407\n",
            "9            1  小明喝了一杯奶茶    一   6.609021\n"
          ]
        }
      ]
    }
  ]
}